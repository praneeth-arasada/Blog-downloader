{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1cf29b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ThreatPost\n",
      "Finished ThreatPost\n",
      "\n",
      "Starting Zdnet\n",
      "Finished Zdnet\n",
      "\n",
      "Starting NakedSecurity\n",
      "Finished NakedSecurity\n",
      "\n",
      "Starting DarkReading\n",
      "Finished DarkReading\n",
      "\n",
      "Starting Grahamcluley\n",
      "Finished Grahamcluley\n",
      "\n",
      "Starting Schneier\n",
      "Finished Schneier\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import os\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import urlopen\n",
    "from xml.etree.ElementTree import parse\n",
    "\n",
    "def ThreatPost_rss():\n",
    "    try:\n",
    "        output_file = r\"C:\\Users\\alpra\\Documents\\blog-down\\ThreatPost\"\n",
    "        files = os.listdir(r\"C:\\Users\\alpra\\Documents\\blog-down\\ThreatPost\")\n",
    "        r = requests.get('https://threatpost.com/feed/')\n",
    "        soup = BeautifulSoup(r.content, features='xml')\n",
    "        articles = soup.findAll('item')\n",
    "        for a in articles:\n",
    "            title = a.find('title').text\n",
    "            title = title.replace('?', ' ')\n",
    "            title = title.replace(':', ' ')\n",
    "            num = 0        \n",
    "            for x in files:\n",
    "                x = x[:-5]\n",
    "                if title == x:\n",
    "                    num+=1\n",
    "                else:\n",
    "                    num == 0\n",
    "            \n",
    "            if num == 0:\n",
    "                link = a.find('link').text\n",
    "                date = a.find('pubDate').text\n",
    "\n",
    "                print(title)\n",
    "                print(date)\n",
    "                print(link)\n",
    "                print()\n",
    "        \n",
    "                # Fetch header first to get check content type\n",
    "                response = requests.head(link)\n",
    "                # Content type can contain encoding information after a semi-colon (`;`), which we're not interested in\n",
    "                content_type = response.headers.get('Content-Type').split(';')[0]\n",
    "\n",
    "                if content_type == 'text/html':\n",
    "                    response = urllib.request.urlopen(link)\n",
    "                    webContent = response.read()\n",
    "                    f = open('{}/{}.html'.format(output_file, title), 'wb')\n",
    "                    f.write(webContent)\n",
    "                    f.close\n",
    "                \n",
    "                time.sleep(1)                \n",
    "                \n",
    "    except Exception as e:\n",
    "        print('Error. See exception:')\n",
    "        print(e) \n",
    "        \n",
    "print('Starting ThreatPost')        \n",
    "ThreatPost_rss()\n",
    "print('Finished ThreatPost')\n",
    "print()\n",
    "\n",
    "\n",
    "def Zdnet_rss():\n",
    "    try:\n",
    "        output_file = r\"C:\\Users\\alpra\\Documents\\blog-down\\Zero Day\"\n",
    "        files = os.listdir(r\"C:\\Users\\alpra\\Documents\\blog-down\\Zero Day\")\n",
    "        var_url = urlopen('https://www.zdnet.com/blog/rss.xml')\n",
    "        xmldoc = parse(var_url)\n",
    "        for item in xmldoc.iterfind('channel/item'):\n",
    "            title = item.findtext('title')\n",
    "            title = title.replace('?', ' ')\n",
    "            title = title.replace(':', ' ')\n",
    "            num = 0        \n",
    "            for x in files:\n",
    "                x = x[:-5]\n",
    "                if title == x:\n",
    "                    num+=1\n",
    "                else:\n",
    "                    num == 0\n",
    "            \n",
    "            if num == 0:\n",
    "                date = item.findtext('pubDate')\n",
    "                link = item.findtext('link')\n",
    "\n",
    "                print(title)\n",
    "                print(date)\n",
    "                print(link)\n",
    "                print()\n",
    "        \n",
    "                # Fetch header first to get check content type\n",
    "                response = requests.head(link)\n",
    "                # Content type can contain encoding information after a semi-colon (`;`), which we're not interested in\n",
    "                content_type = response.headers.get('Content-Type').split(';')[0]\n",
    "\n",
    "                if content_type == 'text/html':\n",
    "                    response = urllib.request.urlopen(link)\n",
    "                    webContent = response.read()\n",
    "                    f = open('{}/{}.html'.format(output_file, title), 'wb')\n",
    "                    f.write(webContent)\n",
    "                    f.close\n",
    "                    \n",
    "                time.sleep(1)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print('Error. See exception:')\n",
    "        print(e) \n",
    "        \n",
    "print('Starting Zdnet')        \n",
    "Zdnet_rss()\n",
    "print('Finished Zdnet')\n",
    "print()\n",
    "\n",
    "\n",
    "def NakedSecurity_rss():\n",
    "    try:\n",
    "        output_file = r\"C:\\Users\\alpra\\Documents\\blog-down\\Naked Security\"\n",
    "        files = os.listdir(r\"C:\\Users\\alpra\\Documents\\blog-down\\Naked Security\")\n",
    "        var_url = urlopen('https://nakedsecurity.sophos.com/feed/')\n",
    "        xmldoc = parse(var_url)\n",
    "        for item in xmldoc.iterfind('channel/item'):\n",
    "            title = item.findtext('title')\n",
    "            title = title.replace('?', ' ')\n",
    "            title = title.replace(':', ' ')\n",
    "            num = 0        \n",
    "            for x in files:\n",
    "                x = x[:-5]\n",
    "                if title == x:\n",
    "                    num+=1\n",
    "                else:\n",
    "                    num == 0\n",
    "            \n",
    "            if num == 0:\n",
    "                date = item.findtext('pubDate')\n",
    "                link = item.findtext('link')\n",
    "\n",
    "                print(title)\n",
    "                print(date)\n",
    "                print(link)\n",
    "                print()\n",
    "        \n",
    "                # Fetch header first to get check content type\n",
    "                response = requests.head(link)\n",
    "                # Content type can contain encoding information after a semi-colon (`;`), which we're not interested in\n",
    "                content_type = response.headers.get('Content-Type').split(';')[0]\n",
    "\n",
    "                if content_type == 'text/html':\n",
    "                    response = urllib.request.urlopen(link)\n",
    "                    webContent = response.read()\n",
    "                    f = open('{}/{}.html'.format(output_file, title), 'wb')\n",
    "                    f.write(webContent)\n",
    "                    f.close\n",
    "                    \n",
    "                time.sleep(1)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print('Error. See exception:')\n",
    "        print(e) \n",
    "        \n",
    "print('Starting NakedSecurity')        \n",
    "NakedSecurity_rss()\n",
    "print('Finished NakedSecurity')\n",
    "print()\n",
    "\n",
    "\n",
    "def DarkReading_rss():\n",
    "    try:\n",
    "        output_file = r\"C:\\Users\\alpra\\Documents\\blog-down\\Dark Reading\"\n",
    "        files = os.listdir(r\"C:\\Users\\alpra\\Documents\\blog-down\\Dark Reading\")\n",
    "        r = requests.get('https://www.darkreading.com/rss_simple.asp?f_n=659&f_ln=Threat%20Intelligence')\n",
    "        soup = BeautifulSoup(r.content, features='xml')\n",
    "        articles = soup.findAll('item')\n",
    "        for a in articles:\n",
    "            title = a.find('title').text\n",
    "            title = title.replace('?', ' ')\n",
    "            title = title.replace(':', ' ')\n",
    "            num = 0        \n",
    "            for x in files:\n",
    "                x = x[:-5]\n",
    "                if title == x:\n",
    "                    num+=1\n",
    "                else:\n",
    "                    num == 0\n",
    "            \n",
    "            if num == 0:\n",
    "                link = a.find('link').text\n",
    "                date = a.find('pubDate').text\n",
    "\n",
    "                print(title)\n",
    "                print(date)\n",
    "                print(link)\n",
    "                print()\n",
    "  \n",
    "                cache_filename = ''\n",
    "                response = requests.get(link)\n",
    "                cache_filename = '{}/{}.html'.format(output_file, title)\n",
    "                with open(cache_filename, mode='w', encoding='utf-8') as cache:\n",
    "                    cache.write(response.text)\n",
    "                    \n",
    "                time.sleep(1) \n",
    "            \n",
    "    except Exception as e:\n",
    "        print('Error. See exception:')\n",
    "        print(e) \n",
    "        \n",
    "print('Starting DarkReading')        \n",
    "DarkReading_rss()\n",
    "print('Finished DarkReading')\n",
    "print()\n",
    "\n",
    "\n",
    "def Grahamcluley_rss():\n",
    "    try:\n",
    "        output_file = r\"C:\\Users\\alpra\\Documents\\blog-down\\GrahamCluley\"\n",
    "        files = os.listdir(r\"C:\\Users\\alpra\\Documents\\blog-down\\GrahamCluley\")\n",
    "        r = requests.get('https://grahamcluley.com/feed/')\n",
    "        soup = BeautifulSoup(r.content, features='xml')\n",
    "        articles = soup.findAll('item')\n",
    "        for a in articles:\n",
    "            title = a.find('title').text\n",
    "            title = title.replace('?', ' ')\n",
    "            title = title.replace(':', ' ')\n",
    "            num = 0        \n",
    "            for x in files:\n",
    "                x = x[:-5]\n",
    "                if title == x:\n",
    "                    num+=1\n",
    "                else:\n",
    "                    num == 0\n",
    "            \n",
    "            if num == 0:\n",
    "                link = a.find('link').text\n",
    "                date = a.find('pubDate').text\n",
    "\n",
    "                print(title)\n",
    "                print(date)\n",
    "                print(link)\n",
    "                print()\n",
    "  \n",
    "                cache_filename = ''\n",
    "                response = requests.get(link)\n",
    "                cache_filename = '{}/{}.html'.format(output_file, title)\n",
    "                with open(cache_filename, mode='w', encoding='utf-8') as cache:\n",
    "                    cache.write(response.text)\n",
    "                    \n",
    "                time.sleep(1)                   \n",
    "                \n",
    "    except Exception as e:\n",
    "        print('Error. See exception:')\n",
    "        print(e) \n",
    "        \n",
    "print('Starting Grahamcluley')        \n",
    "Grahamcluley_rss()\n",
    "print('Finished Grahamcluley')\n",
    "print()\n",
    "\n",
    "def Schneier_rss():\n",
    "    try:\n",
    "        output_file = r\"C:\\Users\\alpra\\Documents\\blog-down\\Schneier\"\n",
    "        files = os.listdir(r\"C:\\Users\\alpra\\Documents\\blog-down\\Schneier\")\n",
    "        r = requests.get('https://www.schneier.com/feed/atom/')\n",
    "        soup = BeautifulSoup(r.content, features='xml')\n",
    "        articles = soup.findAll('entry')\n",
    "        for link in articles:\n",
    "            title = link.find('title').text\n",
    "            title = title.replace('?', ' ')\n",
    "            title = title.replace(':', ' ')\n",
    "            num = 0        \n",
    "            for x in files:\n",
    "                x = x[:-5]\n",
    "                if title == x:\n",
    "                    num+=1\n",
    "                else:\n",
    "                    num == 0\n",
    "            \n",
    "            if num == 0:\n",
    "                date = link.find('updated').text\n",
    "                link = link.find('link')\n",
    "                link = link.get('href')\n",
    "            \n",
    "                print(title)\n",
    "                print(date)\n",
    "                print(link)\n",
    "                print()\n",
    "  \n",
    "                cache_filename = ''\n",
    "                response = requests.get(link)\n",
    "                cache_filename = '{}/{}.html'.format(output_file, title)\n",
    "                with open(cache_filename, mode='w', encoding='utf-8') as cache:\n",
    "                    cache.write(response.text)\n",
    "                    \n",
    "                time.sleep(1)                   \n",
    "                \n",
    "    except Exception as e:\n",
    "        print('Error. See exception:')\n",
    "        print(e) \n",
    "        \n",
    "print('Starting Schneier')        \n",
    "Schneier_rss()\n",
    "print('Finished Schneier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4fb1174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DarkReading\n",
      "Finished DarkReading\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import urlopen\n",
    "from xml.etree.ElementTree import parse\n",
    "\n",
    "def DarkReading_rss():\n",
    "    try:\n",
    "        output_file = r\"C:\\Users\\alpra\\Documents\\blog-down\\Dark Reading\"\n",
    "        var_url = urlopen('https://www.schneier.com/feed/atom/')\n",
    "        xmldoc = parse(var_url)\n",
    "        for item in xmldoc.iterfind('channel/item'):\n",
    "            title = item.findtext('title')\n",
    "            title = title.replace('?', ' ')\n",
    "            title = title.replace(':', ' ')\n",
    "            date = item.findtext('pubDate')\n",
    "            link = item.findtext('link')\n",
    "\n",
    "            print(title)\n",
    "            print(date)\n",
    "            print(link)\n",
    "            print()\n",
    "        \n",
    "            # Fetch header first to get check content type\n",
    "            response = requests.head(link)\n",
    "            # Content type can contain encoding information after a semi-colon (`;`), which we're not interested in\n",
    "            content_type = response.headers.get('Content-Type').split(';')[0]\n",
    "\n",
    "            if content_type == 'text/html':\n",
    "                response = urllib.request.urlopen(link)\n",
    "                webContent = response.read()\n",
    "                f = open('{}/{}.html'.format(output_file, title), 'wb')\n",
    "                f.write(webContent)\n",
    "                f.close\n",
    "                    \n",
    "            time.sleep(1)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print('Error. See exception:')\n",
    "        print(e) \n",
    "        \n",
    "print('Starting DarkReading')        \n",
    "DarkReading_rss()\n",
    "print('Finished DarkReading')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58792202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Grahamcluley\n",
      "Vulnerabilities in Weapons Systems\n",
      "2021-06-08T14:16:31Z\n",
      "https://www.schneier.com/blog/archives/2021/06/vulnerabilities-in-weapons-systems.html\n",
      "\n",
      "The Supreme Court Narrowed the CFAA\n",
      "2021-06-04T11:36:26Z\n",
      "https://www.schneier.com/blog/archives/2021/06/the-supreme-court-narrowed-the-cfaa.html\n",
      "\n",
      "Friday Squid Blogging  Squids in Space\n",
      "2021-06-04T15:04:27Z\n",
      "https://www.schneier.com/blog/archives/2021/06/friday-squid-blogging-squids-in-space.html\n",
      "\n",
      "Security and Human Behavior (SHB) 2021\n",
      "2021-06-03T22:09:08Z\n",
      "https://www.schneier.com/blog/archives/2021/06/security-and-human-behavior-shb-2021.html\n",
      "\n",
      "The DarkSide Ransomware Gang\n",
      "2021-06-02T14:09:56Z\n",
      "https://www.schneier.com/blog/archives/2021/06/the-darkside-ransomware-gang.html\n",
      "\n",
      "Security Vulnerability in Apple’s Silicon “M1” Chip\n",
      "2021-06-02T14:07:24Z\n",
      "https://www.schneier.com/blog/archives/2021/06/security-vulnerability-in-apples-silicon-m1-chip.html\n",
      "\n",
      "Friday Squid Blogging  Underwater Cameras for Observing Squid\n",
      "2021-05-05T16:08:44Z\n",
      "https://www.schneier.com/blog/archives/2021/05/friday-squid-blogging-underwater-cameras-for-observing-squid.html\n",
      "\n",
      "The Misaligned Incentives for Cloud Security\n",
      "2021-05-26T14:56:54Z\n",
      "https://www.schneier.com/blog/archives/2021/05/the-misaligned-incentives-for-cloud-security.html\n",
      "\n",
      "The Story of the 2011 RSA Hack\n",
      "2021-05-26T14:49:35Z\n",
      "https://www.schneier.com/blog/archives/2021/05/the-story-of-the-2011-rsa-hack.html\n",
      "\n",
      "New Disk Wiping Malware Targets Israel\n",
      "2021-05-26T14:33:19Z\n",
      "https://www.schneier.com/blog/archives/2021/05/new-disk-wiping-malware-targets-israel.html\n",
      "\n",
      "Finished Grahamcluley\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import urlopen\n",
    "from xml.etree.ElementTree import parse\n",
    "\n",
    "def Grahamcluley_rss():\n",
    "    try:\n",
    "        output_file = r\"C:\\Users\\alpra\\Documents\\blog-down\\Schneier\"\n",
    "        files = os.listdir(r\"C:\\Users\\alpra\\Documents\\blog-down\\Schneier\")\n",
    "        r = requests.get('https://www.schneier.com/feed/atom/')\n",
    "        soup = BeautifulSoup(r.content, features='xml')\n",
    "        articles = soup.findAll('entry')\n",
    "        for link in articles:\n",
    "            title = link.find('title').text\n",
    "            title = title.replace('?', ' ')\n",
    "            title = title.replace(':', ' ')\n",
    "            num = 0        \n",
    "            for x in files:\n",
    "                x = x[:-5]\n",
    "                if title == x:\n",
    "                    num+=1\n",
    "                else:\n",
    "                    num == 0\n",
    "            \n",
    "            if num == 0:\n",
    "                date = link.find('updated').text\n",
    "                link = link.find('link')\n",
    "                link = link.get('href')\n",
    "            \n",
    "                print(title)\n",
    "                print(date)\n",
    "                print(link)\n",
    "                print()\n",
    "  \n",
    "                cache_filename = ''\n",
    "                response = requests.get(link)\n",
    "                cache_filename = '{}/{}.html'.format(output_file, title)\n",
    "                with open(cache_filename, mode='w', encoding='utf-8') as cache:\n",
    "                    cache.write(response.text)\n",
    "                    \n",
    "                time.sleep(1)                   \n",
    "                \n",
    "    except Exception as e:\n",
    "        print('Error. See exception:')\n",
    "        print(e) \n",
    "        \n",
    "print('Starting Grahamcluley')        \n",
    "Grahamcluley_rss()\n",
    "print('Finished Grahamcluley')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "676dc6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # pulling data\n",
    "from bs4 import BeautifulSoup # xml parsing\n",
    "import json # exporting to files\n",
    "# save function\n",
    "def save_function(article_list):\n",
    "    with open('articles.txt', 'w') as outfile:\n",
    "        json.dump(article_list, outfile)\n",
    "        # scraping function\n",
    "        def hackernews_rss():\n",
    "            article_list = []\n",
    "            try:\n",
    "                # execute my request, parse the data using XML\n",
    "                # parser in BS4\n",
    "                r = requests.get('https://news.ycombinator.com/rss')\n",
    "                soup = BeautifulSoup(r.content, features='xml')\n",
    "                # select only the \"items\" I want from the data\n",
    "                articles = soup.findAll('item')\n",
    "                # for each \"item\" I want, parse it into a list\n",
    "                for a in articles:\n",
    "                    title = a.find('title').text\n",
    "                    link = a.find('link').text\n",
    "                    published = a.find('pubDate').text\n",
    "                    # create an \"article\" object with the data\n",
    "                    # from each \"item\"\n",
    "                    article = {\n",
    "                        'title': title,\n",
    "                        'link': link,\n",
    "                        'published': published\n",
    "                        }\n",
    "                    print(article_list)\n",
    "                # append my \"article_list\" with each \"article\" object\n",
    "                article_list.append(article)\n",
    "                # after the loop, dump my saved objects into a .txt file\n",
    "                return save_function(article_list)\n",
    "            except Exception as e:\n",
    "                print('The scraping job failed. See exception:')\n",
    "                print(e)\n",
    "        print('Starting scraping')\n",
    "        hackernews_rss()\n",
    "        print('Finished scraping')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
