{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eeff5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: ThreatPost\n",
      "\n",
      "Updated: ThreatPost\n",
      "\n",
      "Starting: Zdnet\n",
      "\n",
      "Brazil sees ICT recruitment boom in 2021\n",
      "Wed, 09 Jun 2021 23:40:30 +0000\n",
      "https://www.zdnet.com/article/brazil-sees-ict-recruitment-boom-in-2021/#ftag=RSSbaffb68\n",
      "\n",
      "Facebook expands remote work options\n",
      "Wed, 09 Jun 2021 18:59:54 +0000\n",
      "https://www.zdnet.com/article/facebook-expands-remote-work-options/#ftag=RSSbaffb68\n",
      "\n",
      "Updated: Zdnet\n",
      "\n",
      "Starting: NakedSecurity\n",
      "\n",
      "Updated: NakedSecurity\n",
      "\n",
      "Starting: DarkReading\n",
      "\n",
      "Required MFA Is Not Sufficient for Strong Security  Report\n",
      "Wed, 09 Jun 2021 18:30:00 EDT\n",
      "https://www.darkreading.com/cloud/required-mfa-is-not-sufficient-for-strong-security-report/d/d-id/1341263?_mc=rss_x_drr_edt_aud_dr_x_x-rss-simple\n",
      "\n",
      "CISA Addresses Rise in Ransomware Threatening OT Assets\n",
      "Wed, 09 Jun 2021 17:02:00 EDT\n",
      "https://www.darkreading.com/threat-intelligence/cisa-addresses-rise-in-ransomware-threatening-ot-assets/d/d-id/1341260?_mc=rss_x_drr_edt_aud_dr_x_x-rss-simple\n",
      "\n",
      "Updated: DarkReading\n",
      "\n",
      "Starting: GrahamCluley\n",
      "\n",
      "Updated: GrahamCluley\n",
      "\n",
      "Starting: TechNewsWorld\n",
      "\n",
      "Updated: TechNewsWorld\n",
      "\n",
      "Starting: The Hacker News\n",
      "\n",
      "Beef Supplier JBS Paid Hackers $11 Million Ransom After Cyberattack\n",
      "Wed, 09 Jun 2021 22:46:05 PDT\n",
      "http://feedproxy.google.com/~r/TheHackersNews/~3/1MDhg0pYgZk/beef-supplier-jbs-paid-hackers-11.html\n",
      "\n",
      "New Chrome 0-Day Bug Under Active Attacks â€“ Update Your Browser ASAP!\n",
      "Wed, 09 Jun 2021 21:14:21 PDT\n",
      "http://feedproxy.google.com/~r/TheHackersNews/~3/SQiAlMayhYg/new-chrome-0-day-bug-under-active.html\n",
      "\n",
      "Updated: The Hacker News\n",
      "\n",
      "Starting: Information Security Buzz\n",
      "\n",
      "Updated: Information Security Buzz\n",
      "\n",
      "Starting: Krebs on Security\n",
      "\n",
      "Updated: Krebs on Security\n",
      "\n",
      "Starting: DarkReading\n",
      "\n",
      "Updated: DarkReading\n",
      "\n",
      "Starting: Grahamcluley\n",
      "\n",
      "Updated: Grahamcluley\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from urllib.request import urlopen\n",
    "from xml.etree.ElementTree import parse\n",
    "   \n",
    "  \n",
    "def download_blog_from_xml(dir, url, xmldoc, utf):\n",
    "    try:\n",
    "        output_file = dir\n",
    "        files = os.listdir(dir)\n",
    "        if xmldoc:\n",
    "            var_url = urlopen(url)\n",
    "            xmldoc = parse(var_url)\n",
    "            for item in xmldoc.iterfind('channel/item'):\n",
    "                title = item.findtext('title')\n",
    "                title = title.replace('?', ' ')\n",
    "                title = title.replace(':', ' ')\n",
    "                num = 0        \n",
    "                for x in files:\n",
    "                    x = x[:-5]\n",
    "                    if title == x:\n",
    "                        num+=1\n",
    "                    else:\n",
    "                        num == 0\n",
    "\n",
    "                if num == 0:\n",
    "                    date = item.findtext('pubDate')\n",
    "                    link = item.findtext('link')\n",
    "\n",
    "                    print(title)\n",
    "                    print(date)\n",
    "                    print(link)\n",
    "                    print()\n",
    "                    if utf:\n",
    "                            cache_filename = ''\n",
    "                            response = requests.get(link)\n",
    "                            cache_filename = '{}/{}.html'.format(output_file, title)\n",
    "                            with open(cache_filename, mode='w', encoding='utf-8') as cache:\n",
    "                                cache.write(response.text)\n",
    "                    else: \n",
    "                            # Fetch header first to get check content type\n",
    "                            response = requests.head(link)\n",
    "                            # Content type can contain encoding information after a semi-colon (`;`), which we're not interested in\n",
    "                            content_type = response.headers.get('Content-Type').split(';')[0]\n",
    "\n",
    "                            if content_type == 'text/html':\n",
    "                                response = urllib.request.urlopen(link)\n",
    "                                webContent = response.read()\n",
    "                                f = open('{}/{}.html'.format(output_file, title), 'wb')\n",
    "                                f.write(webContent)\n",
    "                                f.close\n",
    "\n",
    "        else: \n",
    "            r = requests.get(url)\n",
    "            soup = BeautifulSoup(r.content, features='xml')\n",
    "            articles = soup.findAll('item')\n",
    "            for a in articles:\n",
    "                title = a.find('title').text\n",
    "                title = title.replace('?', ' ')\n",
    "                title = title.replace(':', ' ')\n",
    "                num = 0        \n",
    "                for x in files:\n",
    "                    x = x[:-5]\n",
    "                    if title == x:\n",
    "                        num+=1\n",
    "                    else:\n",
    "                        num == 0\n",
    "\n",
    "                if num == 0:\n",
    "                    link = a.find('link').text\n",
    "                    date = a.find('pubDate').text\n",
    "\n",
    "                    print(title)\n",
    "                    print(date)\n",
    "                    print(link)\n",
    "                    print()\n",
    "\n",
    "                    if utf:\n",
    "                            cache_filename = ''\n",
    "                            response = requests.get(link)\n",
    "                            cache_filename = '{}/{}.html'.format(output_file, title)\n",
    "                            with open(cache_filename, mode='w', encoding='utf-8') as cache:\n",
    "                                cache.write(response.text)\n",
    "                    else: \n",
    "                            # Fetch header first to get check content type\n",
    "                            response = requests.head(link)\n",
    "                            # Content type can contain encoding information after a semi-colon (`;`), which we're not interested in\n",
    "                            content_type = response.headers.get('Content-Type').split(';')[0]\n",
    "\n",
    "                            if content_type == 'text/html':\n",
    "                                response = urllib.request.urlopen(link)\n",
    "                                webContent = response.read()\n",
    "                                f = open('{}/{}.html'.format(output_file, title), 'wb')\n",
    "                                f.write(webContent)\n",
    "                                f.close            \n",
    "                \n",
    "    except Exception as e:\n",
    "        print('Error. See exception:')\n",
    "        print(e) \n",
    "    \n",
    "        \n",
    "def function():\n",
    "        article ={1:{\n",
    "                        'title': 'Starting: ThreatPost' ,\n",
    "                        'dir': r'C:\\Users\\alpra\\Documents\\blog-down\\ThreatPost' ,\n",
    "                        'link': 'https://threatpost.com/feed/',\n",
    "                        'xmldoc': False,\n",
    "                        'utf': False,\n",
    "                        'update': 'Updated: ThreatPost'\n",
    "                        }, \n",
    "                  2: {\n",
    "                        'title': 'Starting: Zdnet' ,\n",
    "                        'dir': r'C:\\Users\\alpra\\Documents\\blog-down\\Zero Day' ,\n",
    "                        'link': 'https://www.zdnet.com/blog/rss.xml',\n",
    "                        'xmldoc': True,\n",
    "                        'utf': False,\n",
    "                        'update': 'Updated: Zdnet'\n",
    "                        },\n",
    "                  3: {\n",
    "                        'title': 'Starting: NakedSecurity' ,\n",
    "                        'dir': r'C:\\Users\\alpra\\Documents\\blog-down\\Naked Security' ,\n",
    "                        'link': 'https://nakedsecurity.sophos.com/feed/',\n",
    "                        'xmldoc': True,\n",
    "                        'utf': False,\n",
    "                        'update': 'Updated: NakedSecurity'\n",
    "                        },\n",
    "                  4: {\n",
    "                        'title': 'Starting: DarkReading' ,\n",
    "                        'dir': r'C:\\Users\\alpra\\Documents\\blog-down\\Dark Reading' ,\n",
    "                        'link': 'https://www.darkreading.com/rss_simple.asp?f_n=659&f_ln=Threat%20Intelligence',\n",
    "                        'xmldoc': False,\n",
    "                        'utf': True,\n",
    "                        'update': 'Updated: DarkReading'\n",
    "                        },\n",
    "                  5: {\n",
    "                        'title': 'Starting: GrahamCluley' ,\n",
    "                        'dir': r'C:\\Users\\alpra\\Documents\\blog-down\\GrahamCluley' ,\n",
    "                        'link': 'https://grahamcluley.com/feed/',\n",
    "                        'xmldoc': False,\n",
    "                        'utf': True,\n",
    "                        'update': 'Updated: GrahamCluley'\n",
    "                        },\n",
    "                  6: {\n",
    "                        'title': 'Starting: TechNewsWorld' ,\n",
    "                        'dir': r'C:\\Users\\alpra\\Documents\\blog-down\\Tech News World' ,\n",
    "                        'link': 'https://www.technewsworld.com/perl/syndication/rssfull.pl?__hstc=67659214.41033b4a8973e2312a56265f9f08be64.1623167946568.1623167946568.1623167946568.1&__hssc=67659214.2.1623167946570&__hsfp=4264810217',\n",
    "                        'xmldoc': True,\n",
    "                        'utf': False,\n",
    "                        'update': 'Updated: TechNewsWorld'\n",
    "                        },\n",
    "                  7: {\n",
    "                        'title': 'Starting: The Hacker News' ,\n",
    "                        'dir': r'C:\\Users\\alpra\\Documents\\blog-down\\The Hacker News' ,\n",
    "                        'link': 'https://feeds.feedburner.com/TheHackersNews',\n",
    "                        'xmldoc': True,\n",
    "                        'utf': False,\n",
    "                        'update': 'Updated: The Hacker News'\n",
    "                        },\n",
    "                  8: {\n",
    "                        'title': 'Starting: Information Security Buzz' ,\n",
    "                        'dir': r'C:\\Users\\alpra\\Documents\\blog-down\\Information Security Buzz' ,\n",
    "                        'link': 'https://feeds.feedburner.com/InformationSecurityBuzz',\n",
    "                        'xmldoc': True,\n",
    "                        'utf': False,\n",
    "                        'update': 'Updated: Information Security Buzz'\n",
    "                        },\n",
    "                  9: {\n",
    "                        'title': 'Starting: Krebs on Security' ,\n",
    "                        'dir': r'C:\\Users\\alpra\\Documents\\blog-down\\Krebs on Security' ,\n",
    "                        'link': 'https://krebsonsecurity.com/feed/',\n",
    "                        'xmldoc': False,\n",
    "                        'utf': True,\n",
    "                        'update': 'Updated: Krebs on Security'\n",
    "                        }\n",
    "                 }\n",
    "\n",
    "        for i in range(1,10):\n",
    "            print(article[i]['title']) \n",
    "            print() \n",
    "            download_blog_from_xml(article[i]['dir'], article[i]['link'], article[i]['xmldoc'], article[i]['utf'])\n",
    "            print(article[i]['update'])\n",
    "            print()\n",
    "function()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cfd8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
